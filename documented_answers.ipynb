{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to identify the person who have conducted frauds in the Enron event. In machine learning language, we are trying to classify the persons involved into two categories: persons of interest and persons not of interest. There are many machine learning algorithms that can potentially solve this classification problem based on the features provided, such as the financial features and the email features. In this data set, there are 145 people, and 19 of them have been labeled as person of interest. There are 14 financial features, which show the salaries and related finanical activieis, and 6 email features, which shows the interactions among those 146 people. Among the email features, the 'email_address' is irrelevant and will be deleted. **Thus, 18 features are left.** As shown in poi_id_help file, there are some outliers available in the data set, and they are 'LAY KENNETH L' for both 'loan_advances' and 'total_payments', and 'BHATNAGAR SANJAY' for 'restricted_stock_deferred'. Among those three persons, 'LAY KENNETH L' is a poi, and I do not want to delete this person. Thus, instead of removing rows, I will not use the features that have outliers. This approach is risky in that the outliers or the features with outliers themselves could be strong indicators during the classification, but for now I will take the risk, since we have a lot other features that could do the job. **Thus, so far 16 features left.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I want to use two new features: \n",
    "1. the ratio of 'to_messages' and 'from_messages'\n",
    "2. the ratio of 'from_poi_to_this_person' and 'from_this_person_to_poi'\n",
    "The reason is that pois tend to be more powerful, and they receive more emails than they send.\n",
    "\n",
    "In this way, put together this two new features with the previous 16 features, giving us 18 features. Since different features have different range of values, I rescale all the features to be in [0,1] using \n",
    "\\begin{align}\n",
    "\\textrm{new_array} = \\frac{\\textrm{old_array-min(old_array)}}{\\textrm{max(old_array)-min(old_array)}}\n",
    "\\end{align}\n",
    "\n",
    "I use **SelectKBest** to do the feature selection with the two new features, and the result is\n",
    "```\n",
    "print select.scores_\n",
    "\n",
    "[ 18.57570327\n",
    "   0.21705893\n",
    "   21.06000171\n",
    "   11.59554766\n",
    "   24.46765405\n",
    "   6.23420114\n",
    "   25.09754153\n",
    "   4.20497086\n",
    "   10.07245453\n",
    "   9.34670079\n",
    "   2.10765594\n",
    "   8.74648553\n",
    "   1.69882435\n",
    "   0.1641645\n",
    "   5.34494152\n",
    "   2.42650813\n",
    "   5.55502427\n",
    "   0.42324423]\n",
    "```\n",
    "This scores correspond to the features in the same order as follows\n",
    "```\n",
    "\n",
    " 'salary',                  # feature 1\n",
    " 'deferral_payments',       # feature 2\n",
    " 'bonus',                   # feature 3\n",
    " 'deferred_income',         # feature 4\n",
    " 'total_stock_value',       # feature 5\n",
    " 'expenses',                # feature 6\n",
    " 'exercised_stock_options', # feature 7\n",
    " 'other',                   # feature 8\n",
    " 'long_term_incentive',     # feature 9\n",
    " 'restricted_stock',        # feature 10\n",
    " 'director_fees',           # feature 11\n",
    " 'shared_receipt_with_poi', # feature 12\n",
    " 'to_messages',             # feature 13\n",
    " 'from_messages',           # feature 14\n",
    " 'from_poi_to_this_person', # feature 15\n",
    " 'from_this_person_to_poi'] # feature 16\n",
    " 'new feature 1'            # feature 17\n",
    " 'new feature 2'            # feature 18\n",
    "```\n",
    "It seems that the first new feature (second last in the list)--the ratio of 'to_messages' and 'from_messages'--has a much higher score than the second new feature (last in the list)--the ratio of 'from_poi_to_this_person' and 'from_this_person_to_poi'--. This is a bit surprising, and I do not know why there is this much difference. However, if we look at scores of the original four features, features 13, 14, 15, and 16, it can be seen that features 13 and 14 are not important although their ratio is import, and feature 15 is important although its ratio to feature 16 is not important. We will see what will happen if I delete the two new features, since I will not use them in the final model.\n",
    "\n",
    "Without the two new features, I do the **SelectKBest** again\n",
    "```\n",
    "print select.scores_\n",
    "[ 18.57570327\n",
    "   0.21705893\n",
    "   21.06000171\n",
    "   11.59554766\n",
    "   24.46765405\n",
    "   6.23420114\n",
    "   25.09754153\n",
    "   4.20497086\n",
    "   10.07245453\n",
    "   9.34670079\n",
    "   2.10765594\n",
    "   8.74648553\n",
    "   1.69882435\n",
    "   0.1641645\n",
    "   5.34494152\n",
    "   2.42650813]\n",
    "```\n",
    "This correponds to the first 16 features in the feature list above. It can be seen that features 13, 14, 15 and 16 have roughly the same values as above. I don't know what a proper threshold sould be, and so I will just **try** 5, which left us 10 features. Thus, inside the function SelectKBest, I will set k = 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the three classifiers I have tried, GaussianNB gives the best compared with SVC and RandomForestClassifier. For the GaussianNB, it has a precision of 0.433, a recall of 0.346. However, for example, SVC gives a recall score of 0.012, which is almost zero. This result is strange to me that both SVC and random forest do not work very well, and there must be some hyper parameters that need to be further tuned in order to make these two algorithms to work. I will continuously try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are hyper parameters that are not directly fitted to data, and need to be tuned by hands. Those parameters are vital for the success of the modeling. The algorithm I picked is naive bayes, and it does not need hyper parameters. But for the other two algorithms I tried, such as the random forest, I need to tune the min_samples_split, which represent minimum number of samples that can be used to do another branching. I used GridSearchCV to try different combinations of the hyper parameters, and pick the one giving the best performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation is used to pick the best model through the hyper parameter space and regularization strength. Based on the size of the data set, different strategies should be used. If data is abundant, then one can split the training set into two parts, the first part is used for training, and the second part is used for validation. If data is not that abundant but also not rare, one can use cross validation. However, if data is really rare, then one needs to use stratified split, which is used in my project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metrics I used are precision and recall. For the naive Bayes, I obtain a precision of 0.43, and a recall of 0.35. In the Enron case, a precision of 0.43 means that among 100 predicted pois, 43 of them are really pois; a recall of 0.35 means that if there are 100 true pois, only 35 of them are correctly identified as such."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
